{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Note** \n",
    "\n",
    "Closing this notebook will not save the notebook. Please download it in order to save it. To continue from a saved notebook, you can upload your downloaded notebook continue in your progress. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last notebook, we imported data from a file and took a high-level look at the contents. For this notebook, we are going to jump into the data, exploring it a little bit more and then identifying and cleaning incorrect or missing data. The ultimate goal of this notebook is to get our data to a place where it is ready to be analyzed to answer real-world questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest Data - because we are in a new notebook, we no longer have the data loaded and will need to read it from file again.\n",
    "\n",
    "df = pd.read_csv(r'/home/jovyan/flight_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with data, the datasets that you will receive will usually require some work to get to a state that is ready for analysis. This may be for a variety of reasons such a missing data or data that is in an unusable format or even data is that incorrect. The first step in understanding the data is to explore and handle these unclean and missing portions of data. \n",
    "\n",
    "To do this we are going to see what data is missing using the isnull() method and the sum() method. In effect, the isnull() method identifies null elements (\"cells\" in the table that don't contain any information), and the sum() method will add up all of these nulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Info\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be 3 columns with large numbers of null values: event, departure_airport, and destination_airport. Generally, there are 2 ways in which you can go about working with missing data. Either you can impute - or guessimate - them based off prior knowledge or some statistical technique, or you can remove them.  \n",
    "\n",
    "Let's first take a look at the records that are missing a departure airport. To do this will we need to subset the data with only those that have a null value in the departure airport and destination airport. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ df['departure_airport'].isnull() ].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that every record that is missing a departure airport is also missing a destination airport. Indeed, if you were to check the full data set, you would see that this is true. [Bonus exercise - show that this is true!] If only one or the other was missing then we might be able to fill the missing field using the *airport* field, but since both are missing, there will still be a null field in the record.\n",
    "\n",
    "So imputing the missing data will be challenging. What about deleting the records with missing data? First, we need to determine if records without a departure or destination airport are important to the dataset. If so, we might need to find another source of flight data that we can match to this flight data to fill in the gaps. If not, we can potentially remove those data points if they are not a substantial portion of the dataset, though we need to be confident that we will not add bias to the dataset with their removal.\n",
    "\n",
    "In our case, we are going to assume that we can safely remove these records from the analysis. To do this, we first need to subset the data to grab all those that are not null, and then we can make a new dataset with them. In this case we will use a method that is very similar to the isnull() method, it is called the notnull() method. It does exactly the opposite of the isnull() method. Next, we will subset the data in order and assign it to a new dataframe. Why don't you try doing this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[ df['departure_airport'].notnull() ].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us do a check to see if the null values in the departure airport column were actually removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "answer = new_df.isnull().sum()\n",
    "print(\"Columns and their types:\\n\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to notice is that when we took care of all the missing departure airports, we also took care of all the destination airports. Nice! So let's now move onto the missing events. \n",
    "\n",
    "First thing with the events column is that we are actually going to take a look at what kinds of events there are. We can do this by subsetting the data and then applying the unique() method to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['event'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that there are only 2 events, Off and On. These represent take offs and landings respectively.\n",
    "\n",
    "Given that there are only two unique values and based off the reality that we can actually fill these in based upon the airport column and departure/destination, instead of dropping these columns, let's actually try to impute them. Our logic will be as follows:\n",
    "\n",
    "* If the airport column matches the departure airport then we fill in an \"off\" event\n",
    "* If the airport column matches the destination airport then we will fill in an \"on\" event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that \"&\" means \"and\"\n",
    "on_index = (new_df['event'].isnull()) & (new_df['airport'] == new_df['departure_airport'])\n",
    "new_df.loc[on_index,'event'] = 'off'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, we provide the two conditions for the missing events that we are looking for. They must first be null and secondly, they must be a take off. If these two conditions are met, then we can assign them to a value of 'Off'. You'll notice that there is a &. This is an bitwise operator that indicates that both conditions need to be true in order to return a true value. The usage of .loc in subsetting is just another way to subset the data. It is primarily used for label-based indexing (e.g., usage of the column events). Why don't you try to impute the values of the takeoffs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "off_index = (new_df['event'].isnull()) & (new_df['airport'] == new_df['destination_airport'])\n",
    "new_df.loc[off_index,'event'] = 'on'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that that is done. Let us check to see if we have any last missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, there seems to be a few more missing values. Let's take a look at these rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "index = new_df['event'].isnull() \n",
    "new_df[index].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like neither the departure airport nor the destination airport matches the airport column. We do not have any other information to help diagnose what might be happening with these flights. Given that they represent a very small proportion of the event list, we will remove these data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "index = new_df['event'].notnull()\n",
    "complete_df = new_df[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one last check to see if we are finally done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! We now have a complete dataset to work with. Before we can move on to analyzing it, we need to see if all of the data in the dataset is correct. We will do this in the next section"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
